# FinanceFlow AI

FinanceFlow AI is a Next.js application that uses AI to extract transaction details from text. It provides a simple interface for users to paste transaction descriptions and get back structured data. The application also exposes a REST API for programmatic access.

## Features

- **Multi-Transaction Extraction**: Extracts details from multiple transactions in a single piece of text.
- **Rich Data Extraction**: Extracts a wide range of fields: `description`, `category`, `type`, `amount`, `date`, `merchant`, `paymentMethod`, and `location`.
- **Omnibus Mode**: Allows the AI to assign `null` to fields it cannot confidently determine.
- **Task-Specific Analysis**: Allows users to perform specific tasks, such as categorization or amount extraction (most effective for single-transaction inputs).
- **REST API**: A versioned REST API with authentication for programmatic integration.

## Tech Stack

- **Framework**: [Next.js](https://nextjs.org/)
- **AI**: [Genkit](https://firebase.google.com/docs/genkit) with [OpenRouter](https://openrouter.ai/) fallback
- **Backend**: [Firebase Admin SDK](https://firebase.google.com/docs/admin/setup) for API authentication.
- **Styling**: [Tailwind CSS](https://tailwindcss.com/)
- **UI Components**: [shadcn/ui](https://ui.shadcn.com/)
- **Language**: [TypeScript](https://www.typescriptlang.org/)
- **Testing**: [Vitest](https://vitest.dev/)

## LLM Configuration

FinanceFlow AI uses a multi-provider LLM architecture with automatic fallback capabilities:

### Primary Provider: Genkit (Google AI)
- Uses Google's Gemini 2.0 Flash model via Genkit
- Provides high-quality transaction extraction
- Requires `GOOGLE_API_KEY` environment variable

### Fallback Provider: OpenRouter
- Automatically used when Genkit fails or is unavailable
- Supports multiple open-source models (Gemma, Llama, etc.)
- Requires `OPENROUTER_API_KEY` environment variable
- Can be disabled by setting `LLM_FALLBACK_ENABLED=false`

### Caching System
- In-memory LRU cache for LLM responses
- Reduces API calls and improves response times
- Configurable TTL and cache size
- Can be disabled by setting `LLM_CACHE_ENABLED=false`

### Configuration Options
All LLM settings can be configured via environment variables:

```bash
# Primary provider
GOOGLE_API_KEY=your_google_api_key

# Fallback provider
OPENROUTER_API_KEY=your_openrouter_api_key
LLM_FALLBACK_ENABLED=true

# Caching
LLM_CACHE_ENABLED=true
LLM_CACHE_TTL=3600000          # 1 hour in milliseconds
LLM_CACHE_MAX_SIZE=1000        # Maximum cache entries

# Request settings
LLM_TIMEOUT=30000              # 30 seconds in milliseconds
LLM_MAX_RETRIES=2              # Retries per provider
LLM_RETRY_DELAY=1000           # Delay between retries in milliseconds
```

### Monitoring and Troubleshooting

The application provides detailed logging for LLM operations:
- Provider selection and fallback events
- Cache hit/miss statistics
- Error details with provider information
- Request timing and token usage

Common issues and solutions:
- **All providers failing**: Check API keys and network connectivity
- **Slow responses**: Enable caching or increase timeout values
- **Rate limits**: Configure retry delays or upgrade API plans
- **High costs**: Enable caching and adjust model selection

## Getting Started

### Prerequisites

- Node.js (v20 or higher)
- npm or another package manager

### Installation

1.  **Clone the repository:**

    ```bash
git clone https://github.com/your-username/node-finance-bot-api.git
    cd node-finance-bot-api
```

2.  **Install the dependencies:**

    ```bash
npm install
```

3.  **Set up your environment variables:**

    Create a `.env.local` file by copying the example file:

    ```bash
cp .env.example .env.local
```

    Now, edit `.env.local` to add your environment variables:

    **Required for basic functionality:**
    -   `GOOGLE_API_KEY`: Your Google AI API key for Genkit (primary LLM provider).
    -   `API_DEBUG_MODE_ENABLED`: Set to `true` to disable API authentication for local development.

    **Optional for enhanced reliability:**
    -   `OPENROUTER_API_KEY`: Your OpenRouter API key for fallback LLM provider.
    -   `LLM_FALLBACK_ENABLED`: Set to `true` to enable automatic fallback (default: true).
    -   `LLM_CACHE_ENABLED`: Set to `true` to enable response caching (default: true).

    **Firebase Credentials (required for API endpoints):**
    To run the API endpoints, you need to provide Firebase Admin credentials for App Check verification. Follow the instructions in the `.env.local` file to set this up using either a service account file or environment variables.

    See the [LLM Configuration](#llm-configuration) section above for all available configuration options.

### Running the Development Server

To run the Next.js UI and API, use the following command:

```bash
npm run dev
```

This will start the development server on http://localhost:9002.

To run the Genkit development server for flow debugging, use the following command in a separate terminal:

```bash
npm run genkit:dev
```

This will start the Genkit development server on http://localhost:4000.

## API

The project exposes a versioned REST API for extracting transaction details. For more information, see the [API Implementation Documentation](./docs/api-impl.md).

## Building for Production

To build the application for production, use the following command:

```bash
npm run build
```

This will create a production-ready build in the `.next` directory.

## Running in Production

To run the application in production, use the following command:

```bash
npm start
```

This will start the Next.js production server.

## Linting, Type Checking, and Testing

-   **Linting**: `npm run lint`
-   **Type Checking**: `npm run typecheck`
-   **Testing**: `npm test`


## Running in Production

To run the application in production, use the following command:

```bash
npm start
```

This will start the Next.js production server.

## Linting, Type Checking, and Testing

-   **Linting**: `npm run lint`
-   **Type Checking**: `npm run typecheck`
-   **Testing**: `npm test`
- Import the existing `ai` instance from `src/ai/genkit.ts`
- Implement `LLMProvider` interface
- Convert `LLMMessage[]` format to Genkit's expected format
- Handle the `call()` method by creating a dynamic prompt using `ai.definePrompt()` with the provided messages
- Convert Genkit responses back to `LLMResponse` format
- Wrap Genkit errors in appropriate `LLMProviderError` types
- Support the same Zod schema validation that existing flows expect
- Handle timeout and other options by passing them to Genkit configuration

### src/ai/llm/providers/openrouter.ts(NEW)

References: 

- docs/TODO.md

Implement the OpenRouter provider based on the example in `docs/TODO.md`:

- Implement `LLMProvider` interface
- Use the fetch API to call OpenRouter's `/api/v1/chat/completions` endpoint
- Set up proper headers with `Authorization: Bearer ${process.env.OPENROUTER_API_KEY}`
- Convert `LLMMessage[]` to OpenRouter's expected format (matches OpenAI format)
- Support multiple models in the request payload as shown in the TODO example
- Handle response parsing and error cases (rate limits, API errors, network timeouts)
- Convert OpenRouter responses to `LLMResponse` format
- Implement proper error handling with custom error types
- Add request timeout handling and retry logic for transient failures

### src/ai/llm/cache.ts(NEW)

Create a simple in-memory caching layer for LLM responses:

- Implement an LRU cache using a Map with size limits
- Create cache keys based on message content hash and model parameters
- Define `CacheConfig` interface with ttl (time-to-live), maxSize, and enabled properties
- Implement `LLMCache` class with `get()`, `set()`, and `clear()` methods
- Add cache hit/miss metrics for monitoring
- Support cache invalidation and configurable TTL
- Ensure thread-safety for concurrent requests
- Add optional cache warming for common queries

### src/ai/llm/client.ts(NEW)

References: 

- src/ai/llm/providers/genkit.ts(NEW)
- src/ai/llm/providers/openrouter.ts(NEW)
- src/ai/llm/cache.ts(NEW)

Create the main LLM client that orchestrates providers and fallback logic:

- Import all provider implementations and the cache module
- Define `LLMClientConfig` interface with provider priority, cache settings, and fallback behavior
- Implement `LLMClient` class with a `call()` method that:
  - Checks cache first if enabled
  - Tries providers in order (Genkit first, then OpenRouter)
  - Implements exponential backoff between retries
  - Logs provider failures and fallback attempts
  - Caches successful responses
  - Throws aggregated errors if all providers fail
- Add configuration loading from environment variables
- Support provider health checking and circuit breaker patterns
- Include comprehensive logging for debugging and monitoring

### src/ai/llm/index.ts(NEW)

References: 

- src/ai/llm/types.ts(NEW)
- src/ai/llm/client.ts(NEW)

Create the main export file for the LLM module:

- Export all types from `types.ts`
- Export the `LLMClient` class and create a default configured instance
- Export individual provider classes for testing purposes
- Export cache utilities
- Create a factory function `createLLMClient()` that accepts configuration
- Set up default configuration based on environment variables
- Provide a singleton pattern for the default client instance

### src/ai/prompts/extract-transaction-details.ts(NEW)

References: 

- src/ai/flows/extract-transaction-details.ts(MODIFY)
- docs/TODO.md

Extract the prompt template from the existing flow into a reusable module:

- Move the prompt string from `src/ai/flows/extract-transaction-details.ts` to this dedicated file
- Create a function `buildExtractTransactionDetailsMessages()` that takes input parameters and returns `LLMMessage[]`
- Include both system and user messages as shown in the `docs/TODO.md` example
- Support template variable substitution for dynamic content like `{{text}}`, `{{omnibusMode}}`, etc.
- Ensure the prompt maintains the same functionality as the original Genkit prompt
- Add JSDoc documentation explaining the prompt structure and expected outputs

### src/ai/prompts/handle-missing-transaction-data.ts(NEW)

References: 

- src/ai/flows/handle-missing-transaction-data.ts(MODIFY)

Extract the prompt template from the missing transaction data flow:

- Move the prompt string from `src/ai/flows/handle-missing-transaction-data.ts` to this dedicated file
- Create a function `buildHandleMissingTransactionDataMessages()` that takes input parameters and returns `LLMMessage[]`
- Convert the existing prompt to use system/user message format
- Support template variable substitution for the description parameter
- Maintain the same JSON output format expectations
- Add JSDoc documentation for the prompt structure

### src/ai/flows/extract-transaction-details.ts(MODIFY)

References: 

- src/ai/llm/index.ts(NEW)
- src/ai/prompts/extract-transaction-details.ts(NEW)

Refactor the existing flow to use the new LLM client instead of direct Genkit calls:

- Remove the direct import of `ai` from `src/ai/genkit.ts`
- Import the default LLM client from `src/ai/llm/index.ts`
- Import the prompt builder from `src/ai/prompts/extract-transaction-details.ts`
- Replace the `ai.definePrompt()` and `ai.defineFlow()` pattern with direct LLM client calls
- Update the `extractTransactionDetailsFlow` function to:
  - Build messages using the prompt builder
  - Call `llmClient.call(messages, options)`
  - Parse the JSON response and validate against the existing Zod schema
  - Handle errors and maintain the same error propagation behavior
- Keep all existing TypeScript types and interfaces unchanged
- Maintain the same function signatures so calling code doesn't need changes

### src/ai/flows/handle-missing-transaction-data.ts(MODIFY)

References: 

- src/ai/llm/index.ts(NEW)
- src/ai/prompts/handle-missing-transaction-data.ts(NEW)

Refactor the missing transaction data flow to use the new LLM client:

- Remove the direct import of `ai` from `src/ai/genkit.ts`
- Import the default LLM client from `src/ai/llm/index.ts`
- Import the prompt builder from `src/ai/prompts/handle-missing-transaction-data.ts`
- Replace the Genkit-specific implementation with LLM client calls
- Update the `handleMissingTransactionDataFlow` function to:
  - Build messages using the prompt builder
  - Call `llmClient.call(messages, options)`
  - Parse and validate the response against the existing Zod schema
  - Maintain error handling behavior
- Keep all existing TypeScript types and function signatures unchanged
- Ensure the same output format and validation as the original implementation

### src/lib/errors.ts(MODIFY)

References: 

- src/ai/llm/types.ts(NEW)

Enhance the error handling to support LLM-specific errors:

- Add new error mapping functions for LLM provider errors:
  - `mapLLMProviderErrorToResponse()` for general LLM failures
  - `mapLLMTimeoutErrorToResponse()` for timeout scenarios
  - `mapLLMQuotaExceededErrorToResponse()` for rate limit/quota issues
- Import the LLM error types from `src/ai/llm/types.ts`
- Add appropriate HTTP status codes and RFC 7807 problem details for each error type
- Include provider information in error responses for debugging
- Maintain backward compatibility with existing error handling
- Add logging for LLM provider failures to help with monitoring and debugging

### src/app/actions.ts(MODIFY)

References: 

- src/lib/errors.ts(MODIFY)
- src/ai/flows/extract-transaction-details.ts(MODIFY)

Update the server actions to handle the new LLM error types:

- Import the enhanced error mapping functions from `src/lib/errors.ts`
- Update the `getTransactionDetails()` function to:
  - Catch specific LLM error types (LLMProviderError, LLMTimeoutError, etc.)
  - Map them to appropriate error responses using the new error mapping functions
  - Maintain the existing `ActionResult` interface and error handling pattern
  - Add more detailed error messages that include provider information when available
- Keep the same function signature and return type to maintain compatibility
- Add logging for LLM provider fallback events

### .env.example(MODIFY)

Create an environment variables example file for the new LLM configuration:

- Add `OPENROUTER_API_KEY=your_openrouter_api_key_here` for OpenRouter authentication
- Add `LLM_CACHE_ENABLED=true` to control caching behavior
- Add `LLM_CACHE_TTL=3600` for cache time-to-live in seconds
- Add `LLM_CACHE_MAX_SIZE=1000` for maximum cache entries
- Add `LLM_TIMEOUT=30000` for request timeout in milliseconds
- Add `LLM_FALLBACK_ENABLED=true` to control fallback behavior
- Include comments explaining each variable's purpose and expected values
- Reference the existing Google AI configuration variables if they exist

### README.md(MODIFY)

References: 

- .env.example(MODIFY)

Update the README to document the new LLM fallback system:

- Add a new section "LLM Configuration" explaining the multi-provider setup
- Document the required environment variables for OpenRouter integration
- Explain the fallback behavior (Genkit â†’ OpenRouter)
- Add troubleshooting section for common LLM provider issues
- Include examples of how to configure different providers
- Document the caching behavior and how to disable it if needed
- Add information about monitoring and logging for LLM calls
- Reference the `.env.example` file for configuration examples

### tests/ai/llm/client.test.ts(NEW)

References: 

- tests/setup.ts

Create comprehensive unit tests for the LLM client and fallback behavior:

- Test successful calls with Genkit provider
- Test fallback to OpenRouter when Genkit fails
- Test error handling when all providers fail
- Test caching behavior (cache hits and misses)
- Test timeout handling and retry logic
- Mock both Genkit and OpenRouter providers for isolated testing
- Test configuration loading and validation
- Test provider health checking and circuit breaker behavior
- Use vitest framework consistent with existing tests in `tests/` directory
- Include integration tests that verify the complete flow works end-to-end

### tests/ai/flows/extract-transaction-details.test.ts(NEW)

References: 

- src/ai/flows/extract-transaction-details.test.ts
- src/ai/llm/client.ts(NEW)

Update the existing flow tests to work with the new LLM client architecture:

- Update mocking strategy to mock the LLM client instead of Genkit directly
- Test that the flow correctly handles LLM provider failures and fallbacks
- Verify that the same input/output behavior is maintained after the refactor
- Add tests for different provider scenarios (Genkit success, OpenRouter fallback, etc.)
- Ensure all existing test cases continue to pass
- Add new test cases for error scenarios specific to the multi-provider setup
- Mock the LLM client responses to simulate different provider behaviors